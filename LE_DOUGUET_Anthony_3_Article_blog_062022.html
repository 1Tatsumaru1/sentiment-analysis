<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Benchmarking today's NLP methods</title>
	<style type="text/css">
		html{
			font-family: sans-serif;
			width: 70%;
			margin-left: 15%;
			margin-right: 15%;
		}
		h1{
			width: 100%;
			text-align: center;
			margin-top: 40px;
			margin-bottom: 40px;
		}
		summary{
			font-weight: bolder;
			color: darkgray;
		}
		img{
			max-height: 300px;
			width: 60%;
			margin-left: 20%;
			margin-right: 20%;
			margin-top: 20px;
			margin-bottom: 20px;
		}
		h2{
			color: steelblue;
			margin-top: 35px;
		}
		table{
			table-layout: fixed;
			width: 40%;
			margin-left: 30%;
			margin-right: 30%;
		}
		td, th{
			text-align: center;
		}
		td{
			background-color: lightyellow;
		}
	</style>
</head>
<body>
	<h1>Benchmarking NLP methods for text classification</h1>
	<summary>
		In this article, we discuss 3 different methods to perform text classification with Python. Our work is based on the <a href="https://www.kaggle.com/kazanova/sentiment140">'sentiment140'</a> dataset, which includes 1.6 million Tweets (a nice start üê£). Our goal : determining the dominant mood of each Tweet (either positive or negative). We resort to well-know metrics to compare the models : Accuracy, Recall, AUC.
	</summary>
	<h2>The models</h2>
	<p>
		Are compared here :
		<ul>
			<li>A bag-of-words approach, with a logistic regressor as classifier</li>
			<li>A neural network approach, that we tweeked a little bit to improve the performances</li>
			<li>A transformer known as BERT, the current NLP's state-of-the-art technology</li>
		</ul>
	</p>
	<h2>Data preprocessing</h2>
	<p>
		Here is what our dataset looks like :
	</p>
	<img src="img/tw_head.png">
	<p>
		Each model we are using here requires a different kind of preprocessing :
		<ul>
			<li>
				The bag-of-words method works with... words! We can then stick with the text. However, we need to manually extract the interesting features in order to allow the classification algorithm (LogisticRegressor) to focus on the important words. Hence, our text needs to be fully normalized by means of lemmatization, contraction expansion and punctuation removal.
			</li>
			<img src="img/normalize.png">
			<li>
				For the neural network and transformer approaches, we won't need to normalize since we are going to use pre-trained embedding layers. This saves us some time, a lot of computing ressources, and gives better results since it has been trained on a huge base of examples. However, neural networks and transformers don't deal with texts, only with numbers, so we still have work to do. The operation we need to conduct here is a tokenization : the tokenizer transforms a sentence of words into an array of values, having as many values as the number of words the tokenizer has recognized (equals the number of words it has been trained on). Finally, we pad each vector with "0" in order for them to all have the same length as the biggest sentence in our train dataset (64 words in our example).
			</li>
			<img src="img/tokenize.png">
		</ul>
	</p>
	<p>
		Also very important to take into consideration : the number of words in our vocabulary. Using a CountVectorizer, we have determined that a good number of words to consider here would be 40,000: words that are not recognized are replaced by "0".
	</p>
	<h2>Bag-of-words</h2>
	<p>
		<a href="https://www.geeksforgeeks.org/understanding-tf-idf-term-frequency-inverse-document-frequency/">TF-IDF</a> is a weighting system allowing to determine how significant words are to a text (here to a Tweet) within a corpus (here our dataset).
	</p>
	<img src="img/tfidf.png">
	<p>The implementation made by Scikit-learn returns a sparse matrix that can directly be set as input to a classifier such as LogisticRegressionClassifier. Doing so, and enjoying the luxury of a hyperparameter-tuning via cross-validation since this model is very light-weight, here are the results we obtained with our execution :
	</p>
	<table>
		<tr>
			<th>Accuracy</th>
			<th>Recall</th>
			<th>AUC</th>
		</tr>
		<tr>
			<td>0.774</td>
			<td>0.779</td>
			<td>0.774</td>
		</tr>
	</table>
	<p>
		These results are a good baseline, that we hope will be overmatched by our following methods.
	</p>
	<h2>Neural Networks</h2>
	<p>
		When dealing with neural networks, the number of ways to conduct our work approximates infinity, the reason being that you can add as many layers as you want, tune their parameters, shuffle their order, etc. Hence we kept it very simple, leaving to the reader the possiblity to ellaborate if needed.
	</p>
	<p>The base model is as follow :</p>
	<img src="img/glo_raw.png">
	<p>
		As you can see, there is no superficial layer, we used only what was necessary to reach a prediction :
		<ul>
			<li>
				An embedding layer takes our tokenized sentences as input : 64 is the length of our input vectors, and 200 is the length of the weighted vectors of the pre-trained embedding layer provided by Stanford University (GloVe), meaning that every word recognized by the tokenizer will be coded using a combinaison of 200 values.
			</li>
			<li>
				A GRU (Gated Recurrent Unit) layer is here to provide our network with memory capabilities, increasing the performances related to text analysis, when compared with Feed-Forward (FFN) and Convolutional (CNN) neural networks. As you can see, the size of this layer has been set to match the embedding layer's one.
			</li>
			<li>
				The GRU layer is enclosed in a Bidirectional Layer, allowing the network to look at the sentence in both ways : forwards and backwards, doubling the number of parameters in the process.
			</li>
			<li>
				At the end of our network, a Dense layer uses the values obtained from the Bidirectional GRU and issues a probability between 0 et 1 : the closer it gets to 1, the more our newtork considers the Tweet reflects a positive comment, and conversely.
			</li>
		</ul>
	</p>
	<p>
		Here are the results we obtain when applied to our dataset :
	</p>
	<table>
		<tr>
			<th>Accuracy</th>
			<th>Recall</th>
			<th>AUC</th>
		</tr>
		<tr>
			<td>0.815</td>
			<td>0.797</td>
			<td>0.896</td>
		</tr>
	</table>
	<p>Not bad, right ? The neural networks have indeed outperformed the Bag-of-Words approach. Now let's see what the transformers have to say about this.</p>
	<h2>Transformers</h2>
	<p>
		Transformers are very costly models to develop. Therefore we will only fine-tune a pre-trained model to our needs. Fortunalety, there is a variety of pre-trained models for text classification already available, and even one (BERTweet) that has been trained on Tweets - perfect for us.
	</p>
	<img src="img/bert.png">
	<p>
		BERT models are delivered with the entire artillery: tokenizers that match the chosen model, and more importantly trainers that help you fine-tune for a specific purpose (here: text classification). The trainer we chose only includes encoders. Indeed, the transformers' architecture can include both encoders and decoders, but the latter are only required when your model has to produce a text output (translation, text completion, etc.), and is useless on text classification (the "mere" task of determining probabilities out of the model's weights).
	</p>
	<p>
		The sole fine-tuning of this model to our needs still took about 7 hours to complete on GPU (We said it was costly!). But well, the results speak for themselves :
	</p>
	<table>
		<tr>
			<th>Accuracy</th>
			<th>Recall</th>
			<th>AUC</th>
		</tr>
		<tr>
			<td>0.862</td>
			<td>0.850</td>
			<td>0.862</td>
		</tr>
	</table>
	<h2>Conclusion</h2>
	<p>
		As we have illustrated here, the state-of-the-art NLP approach is the one that performed the best on the task at stake. But you also may want to consider other parameters depending on your other requirements. For instance, if the time of training or the disk space occupied by the trained model are issues for you, you may prefer a more classic neural network method despite its poorer quality of predictions.
	</p>
</body>
</html>